# Medallion Architecture ELT Pipeline (Serverless Databricks)

A hands-on, end-to-end demonstration of the Medallion Architecture data pipeline, implemented in Databricks. It is purposed as a basic demonstration of the medallion pattern, with three layers:

- **Bronze Layer**: ingestion of raw data with Auto Loader
- **Silver Layer**: schema enforcement, standardizing formats, preparing the data for downstream gold layer logic.
- **Gold Layer**: data in this layer needs to encourage analysis and interpretation, with low effort.

```mermaid
    flowchart TD
        A[Raw Files] --> B[Auto Loader]
        B --> C[Bronze Delta]
        C --> D[Transformations]
        D --> E[Silver Delta]
        E --> F[Aggregations / Metrics]
        F --> G[Gold Delta]
```

## Repository Structure

```
medallion-pipeline/
│
├── notebooks/
│ ├── 00_setup_catalog_and_schemas.sql
│ ├── 01_generate_orders.py
│ ├── 02_bronze_ingest_autoloader.py
│ ├── 03_silver_transform.py
│ └── 04_gold_aggregations.py
│
├── workflows/
│ └── medallion_pipeline.json
│
├── sample_data/
│ └── orders_001.json
│ └── orders_002.json
│ └── README.md
│
├── diagrams/
│ └── medallion_architecture.png
│
├── .gitignore
└── README.md
```

- **notebooks/** — All ETL notebooks for Bronze, Silver, and Gold layers, as well as sql for setting up the catalog and python notebook for generating syntatic general orders data and writing in a volume.
- **workflows/** — Databricks serverless job definition (JSON).
- **sample_data/** — Sample of the data generated by 01_generate_orders.py
- **diagrams/** — Architecture diagrams (Mermaid + PNG).

## Implementation and Experience

### Technologies Used

- Databricks
- PySpark
- Auto Loader
- Delta Lake
- Serverless Jobs
- Structured Streaming

#### Databricks

The project was developed and tested on the Databricks Lakehouse Platform. Databricks offers native support for Apache Spark, enabling easy use of PySpark and Spark SQL within notebooks while handling cluster management and optimization on the background.

### PySpark

PySpark's DataFrame API feels pythonic, as if I am interacting with structured and unstructured data that have been mapped to python objects. This sounds like an **Object-Relational Mapping** framework, but it isn't.

Let's take a digression to Pandas. A Pandas DataFrame represents data, and a Pandas DataFrame object is an actual **in-memory wrapper of the data it represents**. With PySpark, we can't say a Spark DataFrame is a python object wrapping data, because if we 'unwrap' the object we actually find a plan for querying the data, to be executed by Apache Spark in a distributed way. In this way, Apache Spark adds another layer of abstraction between python objects and data, which allows for the processing of datasets that far exceed local memory.
